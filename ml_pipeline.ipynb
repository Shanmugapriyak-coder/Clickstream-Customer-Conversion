{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder,FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error,r2_score,mean_squared_error\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.1\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_mapping = {\n",
    "    1:'Australia',\n",
    "    2:'Austria',\n",
    "    3:'Belgium',\n",
    "    4:'British Virgin Islands',\n",
    "    5:'Cayman Islands',\n",
    "    6:'Christmas Island',\n",
    "    7:'Croatia',\n",
    "    8:'Cyprus',\n",
    "    9:'Czech Republic',\n",
    "    10:'Denmark',\n",
    "    11:'Estonia',\n",
    "    12:'unidentified',\n",
    "    13:'Faroe Islands',\n",
    "    14:'Finland',\n",
    "    15:'France',\n",
    "    16:'Germany',\n",
    "    17:'Greece',\n",
    "    18:'Hungary',\n",
    "    19:'Iceland',\n",
    "    20:'India',\n",
    "    21:'Ireland',\n",
    "    22:'Italy',\n",
    "    23:'Latvia',\n",
    "    24:'Lithuania',\n",
    "    25:'Luxembourg',\n",
    "    26:'Mexico',\n",
    "    27:'Netherlands',\n",
    "    28:'Norway',\n",
    "    29:'Poland',\n",
    "    30:'Portugal',\n",
    "    31:'Romania',\n",
    "    32:'Russia',\n",
    "    33:'San Marino',\n",
    "    34:'Slovakia',\n",
    "    35:'Slovenia',\n",
    "    36:'Spain',\n",
    "    37:'Sweden',\n",
    "    38:'Switzerland',\n",
    "    39:'Ukraine',\n",
    "    40:'United Arab Emirates',\n",
    "    41:'United Kingdom',\n",
    "    42:'USA',\n",
    "    43:'biz (.biz)',\n",
    "    44:'com (.com)',\n",
    "    45:'int (.int)',\n",
    "    46:'net (.net)',\n",
    "    47:'org (*.org)'\n",
    "}\n",
    "page1={\n",
    "    1:'trousers',\n",
    "    2:'skirts',\n",
    "    3:'blouses',\n",
    "    4:'sale'\n",
    "}\n",
    "\n",
    "colors={\n",
    "    1:'beige',\n",
    "    2:'black',\n",
    "    3:'blue',\n",
    "    4:'brown',\n",
    "    5:'burgundy',\n",
    "    6:'gray',\n",
    "    7:'green',\n",
    "    8:'navy blue',\n",
    "    9:'of many colors',\n",
    "    10:'olive',\n",
    "    11:'pink',\n",
    "    12:'red',\n",
    "    13:'violet',\n",
    "    14:'white'\n",
    "}\n",
    "\n",
    "locations={\n",
    "    1:'top left',\n",
    "    2:'top in the middle',\n",
    "    3:'top right',\n",
    "    4:'bottom left',\n",
    "    5:'bottom in the middle',\n",
    "    6:'bottom right'\n",
    "\n",
    "}\n",
    "\n",
    "model_photo={\n",
    "    1:'en face',\n",
    "    2:'profile'\n",
    "\n",
    "}\n",
    "\n",
    "clothing_model={\n",
    "\n",
    "    'C20':'C', 'B26':'B','C13':'C','B11':'B','B31':'B','C38':'C',\n",
    "    'C24':'C','C45':'C','B24':'B','A11':'A','P39':'P','P18':'P','P16':'P','P11':'P','A3':'A','P1':'P','A13':'A',\n",
    "    'C26':'C','B17':'B','A7':'A','C12':'C','A2':'A','P2':'P','P4':'P','C18':'C','P3':'P','P43':'P','C41':'C',\n",
    "    'C10':'C','C25' :'C','P60' :'P','P77' :'P','C33' :'C','A10' :'A','B34' :'B','P8' :'P','A25':'A','A6' :'A','B10':'B',\n",
    "    'P12':'P', 'A30':'A', 'C14':'C','C19':'C', 'C40':'C','A8' :'A','A21' :'A','A22' :'A','A5' :'A','C11':'C','A16' :'A',\n",
    "    'A29' :'A','B20' :'A','C5' :'C','P55' :'P','P80' :'P','P51' :'P','B25' :'B','C35' :'C','C2' :'C','C17' :'C',\n",
    "    'P14' :'P','P5':'P', 'A39':'A', 'C7':'C', 'P20':'P', 'P67':'P', 'P49':'P', 'P15':'P', 'C44':'C', 'A14':'A', \n",
    "    'C9':'C', 'P57':'P', 'P7':'P', 'A1':'A','A38':'A', 'B2':'B', 'P25':'P', 'B27':'B', 'P10':'P', 'P72':'P',\n",
    "    'B32':'B', 'A33':'A', 'P17':'P', 'C54':'C', 'C56':'C', 'B4':'B','A4':'A', 'C27':'C', 'A15':'A', 'C4':'C',\n",
    "    'A17':'A', 'A41':'A', 'P62':'P', 'A35':'A', 'P48' :'P','C46':'C', 'C6':'C', 'A18':'A','A37':'A', 'A12':'A',\n",
    "    'P26':'P', 'P63':'P', 'B14':'B', 'C15':'C', 'P40':'P', 'A36':'A', 'B15':'B', 'P34':'P', 'A42':'A', 'C55':'C',\n",
    "    'B21':'B', 'P61':'P', 'C8':'C', 'A9':'A', 'P33':'P', 'B8':'B', 'B23':'B', 'B1':'B', 'B13':'B', 'C53':'C', \n",
    "    'P29':'P', 'C16':'C', 'B6':'B','P73':'P', 'C50':'C', 'B16':'B', 'A20':'A', 'P42':'P', 'P74':'P', 'P35':'P',\n",
    "    'A31':'A', 'A26':'A', 'B30':'B', 'P50':'P', 'A28':'A','A32':'A' ,'C59':'C', 'P75':'P', 'P70':'P', 'C48':'C', \n",
    "    'P47':'P', 'C58':'C', 'P6':'P', 'C51':'C', 'A27':'A', 'P68':'P','C21':'C', 'P38':'P', 'C32':'C', 'C30':'C', 'P23':'P', 'P9':'P',\n",
    "    'P19':'P', 'P65':'P', 'C23':'C', 'B29':'B', 'B28':'B', 'B19':'B', 'C34':'C','C49':'C', 'C57':'C', 'P64':'P', \n",
    "    'B7':'B', 'C52':'C', 'P44':'P','P71':'P', 'P59':'P', 'A23':'A', 'P82':'P', 'P36':'P', 'B12':'B',\n",
    "    'B33':'B', 'B9':'B', 'C1':'C', 'P32':'P', 'C42':'C', 'C36':'C', 'P30':'P', 'P37':'P', 'C43':'C', 'C39':'C', \n",
    "    'P56':'P', 'B3':'B','A34':'A', 'P76':'P', 'B22':'B', 'A43':'A', 'C3':'C', 'P13':'P', 'B5':'B', 'C28':'C',\n",
    "    'A40':'A', 'C22':'C', 'C47':'C', 'C29':'C','P24':'P', 'A24':'A', 'P58':'P', 'A19':'A', 'P53':'P', 'C37':'C', \n",
    "    'P46':'P', 'P69':'P', 'C31':'C', 'P45':'P', 'P52':'P', 'P78':'P','P21':'P', 'P81':'P', 'P41':'P', 'P66':'P', \n",
    "    'P27':'P', 'P31' :'P','P79' :'P','P22':'P', 'P54':'P'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "onehot_cols = ['country','page1_main_category','colour','location','model_photography','page2_clothing_model']\n",
    "allowed_countries = ['Poland', 'Czech Republic', 'Lithuania', 'net (.net)', 'com (.com)']\n",
    "\n",
    "def replace_country_func(df):\n",
    "    df = df.copy()\n",
    "    df['country'] = df['country'].map(country_mapping)\n",
    "    return df\n",
    "\n",
    "def replace_page1_func(df):\n",
    "    df = df.copy()\n",
    "    df['page1_main_category'] = df['page1_main_category'].map(page1)\n",
    "    return df\n",
    "\n",
    "def replace_colour_func(df):\n",
    "    df = df.copy()\n",
    "    df['colour'] = df['colour'].map(colors)\n",
    "    return df\n",
    "\n",
    "def replace_location_func(df):\n",
    "    df = df.copy()\n",
    "    df['location'] = df['location'].map(locations)\n",
    "    return df\n",
    "\n",
    "def replace_model_photo_func(df):\n",
    "    df = df.copy()\n",
    "    df['model_photography'] = df['model_photography'].map(model_photo)\n",
    "    return df\n",
    "\n",
    "def replace_clothing_photo_func(df):\n",
    "    df = df.copy()\n",
    "    df['page2_clothing_model'] = df['page2_clothing_model'].map(clothing_model)\n",
    "    return df\n",
    "\n",
    "def filter_countries_func(df):\n",
    "    df = df.copy()\n",
    "    df['country'] = df['country'].apply(\n",
    "        lambda c: c if c in allowed_countries else 'Others'\n",
    "    )\n",
    "    return df\n",
    "\n",
    "scaler = ColumnTransformer([\n",
    "    ('MinMaxScaler',  MinMaxScaler(),['month']),\n",
    "    ('StandardScaler',  StandardScaler(),['day','order']),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore',sparse_output=False), onehot_cols),\n",
    "], remainder='passthrough')\n",
    "\n",
    "# ('replace_country', FunctionTransformer(lambda x: x.assign(country=x['country'].map(country_mapping)), validate=False)),\n",
    "#     ('replace_maincate', FunctionTransformer(lambda x: x.assign(page1_main_category=x['page1_main_category'].map(page1)), validate=False)),\n",
    "#     ('replace_color', FunctionTransformer(lambda x: x.assign(colour=x['colour'].map(colors)), validate=False)),\n",
    "#     ('replace_location', FunctionTransformer(lambda x: x.assign(location=x['location'].map(locations)), validate=False)),\n",
    "#     ('replace_model', FunctionTransformer(lambda x: x.assign(model_photography=x['model_photography'].map(model_photo)), validate=False)),\n",
    "#     ('replace_clothmodel', FunctionTransformer(lambda x: x.assign(page2_clothing_model=x['page2_clothing_model'].map(clothing_model)), validate=False)),\n",
    "#     ('filter_countries', FunctionTransformer(lambda x: x.assign(country=x['country'].apply(lambda c: c if c in allowed_countries else 'Others')), validate=False)),\n",
    "\n",
    "preprocessor = Pipeline([\n",
    "    ('replace_country', FunctionTransformer(replace_country_func, validate=False)),\n",
    "    ('replace_maincate', FunctionTransformer(replace_page1_func, validate=False)),\n",
    "    ('replace_color', FunctionTransformer(replace_colour_func, validate=False)),\n",
    "    ('replace_location', FunctionTransformer(replace_location_func, validate=False)),\n",
    "    ('replace_model', FunctionTransformer(replace_model_photo_func, validate=False)),\n",
    "    ('replace_clothmodel', FunctionTransformer(replace_clothing_photo_func, validate=False)),\n",
    "    ('filter_countries', FunctionTransformer(filter_countries_func, validate=False)),\n",
    "    ('scaler',  scaler) \n",
    "])\n",
    "\n",
    "\n",
    "full_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('model', GradientBoostingRegressor(n_estimators=50,max_depth=5,learning_rate=0.1))\n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********\n",
      "MSE for Training Data : 45.14943536384827\n",
      "MSE for Test data: 44.52394027778663\n",
      "R2 score for train data: 0.712989211302602\n",
      "R2 score for test data: 0.7162958147466025\n",
      "MAE for Training Data : 4.759208091761679\n",
      "MAE for Test data: 4.7454507578177205\n",
      "RMSE for training data 6.719332955275268\n",
      "RMSE for test data 6.67262619047303\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(r\"C:\\Users\\MY Laptop\\Desktop\\guvi_class\\Customer Conversion Analysis\\train_data.csv\")\n",
    "\n",
    "X=df.drop(['price','price_2','session_id'],axis=1)\n",
    "y=df['price']\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\n",
    "full_pipeline.fit(x_train, y_train)\n",
    "train_pred=full_pipeline.predict(x_train)\n",
    "test_pred=full_pipeline.predict(x_test)\n",
    "print(f\"***********\")\n",
    "print(f\"MSE for Training Data : {mean_squared_error(y_train,train_pred)}\")\n",
    "print(f\"MSE for Test data: {mean_squared_error(y_test,test_pred)}\")\n",
    "\n",
    "print(f\"R2 score for train data: {r2_score(y_train,train_pred)}\")\n",
    "print(f\"R2 score for test data: {r2_score(y_test,test_pred)}\")\n",
    "\n",
    "print(f\"MAE for Training Data : {mean_absolute_error(y_train,train_pred)}\")\n",
    "print(f\"MAE for Test data: {mean_absolute_error(y_test,test_pred)}\")\n",
    "\n",
    "print(\"RMSE for training data\", np.sqrt(mean_squared_error(y_train, train_pred)))\n",
    "print(\"RMSE for test data\", np.sqrt(mean_squared_error(y_test, test_pred)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['regression_model.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(full_pipeline, \"regression_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****Train******\n",
      "Accuracy: 0.9756947395257924\n",
      "Precision: 0.9758474576271187\n",
      "Recall: 0.9767111668387668\n",
      "F1 score: 0.9762791212031849\n",
      "ROC AUC: 0.9756695506350364\n",
      "*****Test******\n",
      "Accuracy: 0.9190965402628796\n",
      "Precision: 0.918450184501845\n",
      "Recall: 0.9230826286901054\n",
      "F1 score: 0.9207605800532702\n",
      "ROC AUC: 0.9190216899030025\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,roc_auc_score,roc_curve\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classification_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('model',RandomForestClassifier(random_state=42,class_weight='balanced'))\n",
    "])\n",
    "\n",
    "df=pd.read_csv(r\"C:\\Users\\MY Laptop\\Desktop\\guvi_class\\Customer Conversion Analysis\\train_data.csv\")\n",
    "\n",
    "X=df.drop(['price_2','price','session_id'],axis=1)\n",
    "y=df['price_2']\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "classification_pipeline.fit(x_train, y_train)\n",
    "train_pred=classification_pipeline.predict(x_train)\n",
    "test_pred=classification_pipeline.predict(x_test)\n",
    "print(\"****Train******\")\n",
    "print(f\"Accuracy: {accuracy_score(y_train,train_pred)}\")\n",
    "print(f\"Precision: {precision_score(y_train,train_pred)}\")\n",
    "print(f\"Recall: {recall_score(y_train,train_pred)}\")\n",
    "print(f\"F1 score: {f1_score(y_train,train_pred)}\")\n",
    "print(f\"ROC AUC: {roc_auc_score(y_train,train_pred)}\")\n",
    "print(\"*****Test******\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test,test_pred)}\")\n",
    "print(f\"Precision: {precision_score(y_test,test_pred)}\")\n",
    "print(f\"Recall: {recall_score(y_test,test_pred)}\")\n",
    "print(f\"F1 score: {f1_score(y_test,test_pred)}\")\n",
    "print(f\"ROC AUC: {roc_auc_score(y_test,test_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['classification_model.pkl']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(classification_pipeline, \"classification_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Test******\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 score: 1.0\n",
      "ROC AUC: 1.0\n"
     ]
    }
   ],
   "source": [
    "test_df=pd.read_csv(r\"C:\\Users\\MY Laptop\\Desktop\\guvi_class\\Customer Conversion Analysis\\test_data.csv\")\n",
    "model = joblib.load(\"classification_model.pkl\")\n",
    "y_test2=test_df['price_2']\n",
    "X_test2=test_df.drop(['price_2'],axis=1)\n",
    "\n",
    "ntrain_pred=model.predict(X_test2)\n",
    "print(\"*****Test******\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test2,ntrain_pred)}\")\n",
    "print(f\"Precision: {precision_score(y_test2,ntrain_pred)}\")\n",
    "print(f\"Recall: {recall_score(y_test2,ntrain_pred)}\")\n",
    "print(f\"F1 score: {f1_score(y_test2,ntrain_pred)}\")\n",
    "print(f\"ROC AUC: {roc_auc_score(y_test2,ntrain_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cluster model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=2)),\n",
    "    ('kmeans', KMeans(n_clusters=k, random_state=42))\n",
    "])\n",
    "\n",
    "clusters = pipeline.named_steps['kmeans'].labels_\n",
    "\n",
    "# Calculate silhouette score\n",
    "score = silhouette_score(pipeline.named_steps['pca'].transform(\n",
    "    pipeline.named_steps['scaler'].transform(X)), clusters)\n",
    "\n",
    "print(\"Silhouette Score:\", score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
